{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e644b9ec",
   "metadata": {},
   "source": [
    "## Generative AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a47e0a",
   "metadata": {},
   "source": [
    "## Transformer & LSTM Implementation for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02ccd0",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64121499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders, processors\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f5d75",
   "metadata": {},
   "source": [
    "#### Loading Dataset + Preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8292f1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Aligned Sentence Pairs: 15072\n",
      "Source Vocab Size (English): 8000\n",
      "Target Vocab Size (Urdu): 8000\n",
      "Total Tokenized Sentence Pairs: 15072\n",
      "\n",
      "Sample English Sentence: ﻿The book of the generation of Jesus Christ , the son of David , the son of Abraham .\n",
      "Sample Urdu Sentence: ﻿یسُوع مسیح ابن داود ابن ابرہام کا نسب نامہ\n",
      "ابراہام سے اِضحاق پیدا ہُوا اور اِضحاق سے یعقوب پیدا ہُوا اور یعقوب سے یہوداہ اور اس کے بھائی پیدا ہوئے\n",
      "Sample English Token IDs: [2, 49, 51, 398, 64, 51, 2428, 64, 291, 422, 10, 51, 379, 64, 1475, 10, 51, 379, 64, 705]\n",
      "Sample Urdu Token IDs: [2, 77, 276, 373, 1030, 1258, 1030, 1475, 103, 5183, 2183, 5621, 88, 3029, 345, 274, 81, 3029, 88, 1072]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 1: Load your text data\n",
    "data_dir = 'q1_data'\n",
    "raw_data = {'english': {}, 'urdu': {}}\n",
    "file_map = {'bible': {}, 'quran': {}}\n",
    "\n",
    "# Define a function to correctly identify English and Urdu files\n",
    "def identify_language(filename):\n",
    "    if filename in ['Bible-EN', 'Quran-EN']:\n",
    "        return 'english'\n",
    "    elif filename in ['Bible-UR', 'Quran-UR']:\n",
    "        return 'urdu'\n",
    "    elif filename in ['Bible-UR-normalized', 'Quran-UR-normalized']:\n",
    "        return 'urdu'\n",
    "    elif filename.endswith('.en'):\n",
    "        return 'english'\n",
    "    elif filename.endswith('.ur'):\n",
    "        return 'urdu'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define a simple Urdu sentence splitter\n",
    "def split_sentences(text, lang='english'):\n",
    "    if lang == 'english':\n",
    "        return nltk.sent_tokenize(text)\n",
    "    else:\n",
    "        # Rough Urdu splitting\n",
    "        text = text.replace('؟', '.').replace('۔', '.').replace('!', '.')\n",
    "        return [s.strip() for s in text.split('.') if s.strip()]\n",
    "\n",
    "# Load Bible and Quran data\n",
    "for category in ['bible', 'quran']:\n",
    "    folder_path = os.path.join(data_dir, category)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lang = identify_language(filename)\n",
    "        \n",
    "        if lang:\n",
    "            raw_data[lang][filename] = split_sentences(content, lang=lang)\n",
    "            file_map[category][filename] = lang\n",
    "        else:\n",
    "            print(f\"Unrecognized file: {filename}\")\n",
    "\n",
    "# Step 2: Write raw corpora for training BPE\n",
    "os.makedirs('tokenizer_corpus', exist_ok=True)\n",
    "\n",
    "# Flatten all sentences into one file for tokenizer training\n",
    "with open('tokenizer_corpus/english.txt', 'w', encoding='utf-8') as f:\n",
    "    all_english_sentences = sum(raw_data['english'].values(), [])\n",
    "    f.write('\\n'.join(all_english_sentences))\n",
    "\n",
    "with open('tokenizer_corpus/urdu.txt', 'w', encoding='utf-8') as f:\n",
    "    all_urdu_sentences = sum(raw_data['urdu'].values(), [])\n",
    "    f.write('\\n'.join(all_urdu_sentences))\n",
    "\n",
    "# Step 3: Define BPE tokenizer training function\n",
    "def train_bpe_tokenizer(file_path, vocab_size=8000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    tokenizer.decoder = decoders.BPEDecoder()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train([file_path], trainer)\n",
    "    return tokenizer\n",
    "\n",
    "# Step 4: Train tokenizers\n",
    "english_tokenizer = train_bpe_tokenizer(\"tokenizer_corpus/english.txt\")\n",
    "urdu_tokenizer = train_bpe_tokenizer(\"tokenizer_corpus/urdu.txt\")\n",
    "\n",
    "# Step 5: Align English–Urdu sentence pairs\n",
    "aligned_sentence_pairs = []\n",
    "\n",
    "for category in ['bible', 'quran']:\n",
    "    files = list(file_map[category].keys())\n",
    "    eng_files = [f for f in files if file_map[category][f] == 'english']\n",
    "    urd_files = [f for f in files if file_map[category][f] == 'urdu']\n",
    "\n",
    "    for eng_file, urd_file in zip(sorted(eng_files), sorted(urd_files)):\n",
    "        eng_sentences = raw_data['english'][eng_file]\n",
    "        urd_sentences = raw_data['urdu'][urd_file]\n",
    "\n",
    "        min_len = min(len(eng_sentences), len(urd_sentences))\n",
    "\n",
    "        for i in range(min_len):\n",
    "            eng_sent = eng_sentences[i]\n",
    "            urd_sent = urd_sentences[i]\n",
    "\n",
    "            if len(eng_sent.split()) < 5 or len(urd_sent.split()) < 5:\n",
    "                continue\n",
    "            if len(eng_sent.split()) > 50 or len(urd_sent.split()) > 50:\n",
    "                continue\n",
    "\n",
    "            aligned_sentence_pairs.append((eng_sent, urd_sent))\n",
    "\n",
    "print(f\"\\nTotal Aligned Sentence Pairs: {len(aligned_sentence_pairs)}\")\n",
    "\n",
    "# Step 6: Tokenize aligned sentence pairs\n",
    "def tokens_to_ids(tokenizer, tokens):\n",
    "    return [tokenizer.token_to_id(\"[CLS]\")] + \\\n",
    "           [tokenizer.token_to_id(t) for t in tokens if tokenizer.token_to_id(t) is not None] + \\\n",
    "           [tokenizer.token_to_id(\"[SEP]\")]\n",
    "\n",
    "tokenized_pairs = []\n",
    "\n",
    "for eng_sent, urd_sent in aligned_sentence_pairs:\n",
    "    eng_encoded = english_tokenizer.encode(eng_sent)\n",
    "    urd_encoded = urdu_tokenizer.encode(urd_sent)\n",
    "    src_ids = tokens_to_ids(english_tokenizer, eng_encoded.tokens)\n",
    "    tgt_ids = tokens_to_ids(urdu_tokenizer, urd_encoded.tokens)\n",
    "    tokenized_pairs.append((src_ids, tgt_ids))\n",
    "\n",
    "# Step 7: Get Vocab Sizes\n",
    "src_vocab_size = english_tokenizer.get_vocab_size()\n",
    "tgt_vocab_size = urdu_tokenizer.get_vocab_size()\n",
    "\n",
    "print(\"Source Vocab Size (English):\", src_vocab_size)\n",
    "print(\"Target Vocab Size (Urdu):\", tgt_vocab_size)\n",
    "print(\"Total Tokenized Sentence Pairs:\", len(tokenized_pairs))\n",
    "\n",
    "# Step 8: Sample\n",
    "print(\"\\nSample English Sentence:\", aligned_sentence_pairs[0][0])\n",
    "print(\"Sample Urdu Sentence:\", aligned_sentence_pairs[0][1])\n",
    "print(\"Sample English Token IDs:\", tokenized_pairs[0][0][:20])\n",
    "print(\"Sample Urdu Token IDs:\", tokenized_pairs[0][1][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb8871",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb9499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, v)\n",
    "    return output, attn\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0, \"d_model must be divisible by heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        if y is None:\n",
    "            y = x\n",
    "\n",
    "        B, L_x, _ = x.size()\n",
    "        L_y = y.size(1)\n",
    "\n",
    "        q = self.q_linear(x).view(B, L_x, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # For multi-heads\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, v)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_x, self.d_model)\n",
    "        out = self.fc_out(context)\n",
    "        return out\n",
    "\n",
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.cross_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), mask=tgt_mask))\n",
    "        x = x + self.dropout(self.cross_attn(self.norm2(x), y=enc_out, mask=src_mask))\n",
    "        x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        x = self.embed(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = self.embed(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Full Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        out = self.fc_out(dec_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82703456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: Dataset Preparation and Hyperparameter Configuration\n",
    "# Hyperparameter configuration\n",
    "config = {\n",
    "    'd_model': 256,          # Reduced from 512 for efficiency\n",
    "    'n_layers': 4,           # Reduced from 6\n",
    "    'heads': 8,\n",
    "    'd_ff': 1024,            # Reduced from 2048\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.0001,            # Lower learning rate\n",
    "    'epochs': 30,\n",
    "    'max_seq_len': 100,      # Maximum sequence length\n",
    "    'warmup_steps': 4000,    # For learning rate scheduling\n",
    "    'print_every': 50,\n",
    "    'label_smoothing': 0.1,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_pad_idx, tgt_pad_idx, split='train'):\n",
    "        self.pairs = pairs\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.split = split  # 'train', 'val', or 'test'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        return torch.LongTensor(src), torch.LongTensor(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1c7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Padding values\n",
    "src_pad_idx = english_tokenizer.token_to_id(\"[PAD]\")\n",
    "tgt_pad_idx = urdu_tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "# Define collate function for padding and truncation\n",
    "def collate_fn(batch, src_pad_idx, tgt_pad_idx):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    \n",
    "    # Truncate sequences to max_seq_len\n",
    "    src_batch = [seq[:config['max_seq_len']] for seq in src_batch]\n",
    "    tgt_batch = [seq[:config['max_seq_len']] for seq in tgt_batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_batch = pad_sequence(src_batch, padding_value=src_pad_idx, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
    "    \n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Train-test split\n",
    "train_pairs, val_pairs = train_test_split(tokenized_pairs, test_size=0.1, random_state=42)\n",
    "train_pairs, test_pairs = train_test_split(train_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TranslationDataset(train_pairs, src_pad_idx, tgt_pad_idx, split='train')\n",
    "val_dataset = TranslationDataset(val_pairs, src_pad_idx, tgt_pad_idx, split='val')\n",
    "test_dataset = TranslationDataset(test_pairs, src_pad_idx, tgt_pad_idx, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], collate_fn=lambda batch: collate_fn(batch, src_pad_idx, tgt_pad_idx))\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], collate_fn=lambda batch: collate_fn(batch, src_pad_idx, tgt_pad_idx))\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], collate_fn=lambda batch: collate_fn(batch, src_pad_idx, tgt_pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d12a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def lcs(X, Y):\n",
    "    # Helper function for ROUGE-L\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    L = [[0] * (n+1) for i in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i] == Y[j]:\n",
    "                L[i+1][j+1] = L[i][j] + 1\n",
    "            else:\n",
    "                L[i+1][j+1] = max(L[i+1][j], L[i][j+1])\n",
    "    return L[m][n]\n",
    "\n",
    "def rouge_l_score(pred_tokens, ref_tokens):\n",
    "    lcs_len = lcs(pred_tokens, ref_tokens)\n",
    "    precision = lcs_len / len(pred_tokens) if pred_tokens else 0\n",
    "    recall = lcs_len / len(ref_tokens) if ref_tokens else 0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def evaluate_bleu_rouge_nltk(model, loader, english_tokenizer, urdu_tokenizer, device, tgt_pad_idx):\n",
    "    model.eval()\n",
    "    smooth_fn = SmoothingFunction().method4\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            output = model(src, tgt_input)\n",
    "            output_ids = output.argmax(dim=-1)\n",
    "            \n",
    "            for pred, target in zip(output_ids, tgt[:, 1:]):\n",
    "                pred_tokens = [urdu_tokenizer.id_to_token(id.item()) for id in pred if id.item() != tgt_pad_idx]\n",
    "                target_tokens = [urdu_tokenizer.id_to_token(id.item()) for id in target if id.item() != tgt_pad_idx]\n",
    "                \n",
    "                bleu = sentence_bleu(\n",
    "                    [target_tokens], pred_tokens, smoothing_function=smooth_fn\n",
    "                )\n",
    "                rouge_l = rouge_l_score(pred_tokens, target_tokens)\n",
    "                \n",
    "                bleu_scores.append(bleu)\n",
    "                rouge_l_scores.append(rouge_l)\n",
    "    \n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "    return avg_bleu, avg_rouge_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc13b311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 7.5647 | Val Loss: 6.7014\n",
      "BLEU Score: 0.0087 | ROUGE-L Score: 0.0751\n",
      "Epoch 2 | Train Loss: 6.3374 | Val Loss: 6.0659\n",
      "BLEU Score: 0.0168 | ROUGE-L Score: 0.1541\n",
      "Epoch 3 | Train Loss: 5.8271 | Val Loss: 5.6431\n",
      "BLEU Score: 0.0233 | ROUGE-L Score: 0.1987\n",
      "Epoch 4 | Train Loss: 5.4067 | Val Loss: 5.2268\n",
      "BLEU Score: 0.0358 | ROUGE-L Score: 0.2475\n",
      "Epoch 5 | Train Loss: 5.0202 | Val Loss: 4.8731\n",
      "BLEU Score: 0.0506 | ROUGE-L Score: 0.2859\n",
      "Epoch 6 | Train Loss: 4.6858 | Val Loss: 4.5686\n",
      "BLEU Score: 0.0672 | ROUGE-L Score: 0.3158\n",
      "Epoch 7 | Train Loss: 4.3878 | Val Loss: 4.2914\n",
      "BLEU Score: 0.0853 | ROUGE-L Score: 0.3418\n",
      "Epoch 8 | Train Loss: 4.1089 | Val Loss: 4.0306\n",
      "BLEU Score: 0.1050 | ROUGE-L Score: 0.3660\n",
      "Epoch 9 | Train Loss: 3.8577 | Val Loss: 3.7994\n",
      "BLEU Score: 0.1250 | ROUGE-L Score: 0.3878\n",
      "Epoch 10 | Train Loss: 3.6360 | Val Loss: 3.5891\n",
      "BLEU Score: 0.1457 | ROUGE-L Score: 0.4080\n",
      "Epoch 11 | Train Loss: 3.4329 | Val Loss: 3.4071\n",
      "BLEU Score: 0.1659 | ROUGE-L Score: 0.4262\n",
      "Epoch 12 | Train Loss: 3.2530 | Val Loss: 3.2446\n",
      "BLEU Score: 0.1857 | ROUGE-L Score: 0.4424\n",
      "Epoch 13 | Train Loss: 3.0972 | Val Loss: 3.1058\n",
      "BLEU Score: 0.2034 | ROUGE-L Score: 0.4557\n",
      "Epoch 14 | Train Loss: 2.9618 | Val Loss: 2.9833\n",
      "BLEU Score: 0.2186 | ROUGE-L Score: 0.4669\n",
      "Epoch 15 | Train Loss: 2.8473 | Val Loss: 2.8788\n",
      "BLEU Score: 0.2314 | ROUGE-L Score: 0.4766\n",
      "Epoch 16 | Train Loss: 2.7473 | Val Loss: 2.7920\n",
      "BLEU Score: 0.2435 | ROUGE-L Score: 0.4851\n",
      "Epoch 17 | Train Loss: 2.6617 | Val Loss: 2.7167\n",
      "BLEU Score: 0.2535 | ROUGE-L Score: 0.4923\n",
      "Epoch 18 | Train Loss: 2.5909 | Val Loss: 2.6532\n",
      "BLEU Score: 0.2615 | ROUGE-L Score: 0.4978\n",
      "Epoch 19 | Train Loss: 2.5301 | Val Loss: 2.6000\n",
      "BLEU Score: 0.2696 | ROUGE-L Score: 0.5032\n",
      "Epoch 20 | Train Loss: 2.4799 | Val Loss: 2.5574\n",
      "BLEU Score: 0.2750 | ROUGE-L Score: 0.5070\n",
      "Epoch 21 | Train Loss: 2.4393 | Val Loss: 2.5248\n",
      "BLEU Score: 0.2791 | ROUGE-L Score: 0.5099\n",
      "Epoch 22 | Train Loss: 2.4061 | Val Loss: 2.4964\n",
      "BLEU Score: 0.2829 | ROUGE-L Score: 0.5125\n",
      "Epoch 23 | Train Loss: 2.3788 | Val Loss: 2.4754\n",
      "BLEU Score: 0.2862 | ROUGE-L Score: 0.5148\n",
      "Epoch 24 | Train Loss: 2.3585 | Val Loss: 2.4581\n",
      "BLEU Score: 0.2889 | ROUGE-L Score: 0.5166\n",
      "Epoch 25 | Train Loss: 2.3412 | Val Loss: 2.4455\n",
      "BLEU Score: 0.2908 | ROUGE-L Score: 0.5177\n",
      "Epoch 26 | Train Loss: 2.3306 | Val Loss: 2.4370\n",
      "BLEU Score: 0.2921 | ROUGE-L Score: 0.5185\n",
      "Epoch 27 | Train Loss: 2.3230 | Val Loss: 2.4321\n",
      "BLEU Score: 0.2929 | ROUGE-L Score: 0.5190\n",
      "Epoch 28 | Train Loss: 2.3180 | Val Loss: 2.4285\n",
      "BLEU Score: 0.2934 | ROUGE-L Score: 0.5194\n",
      "Epoch 29 | Train Loss: 2.3139 | Val Loss: 2.4274\n",
      "BLEU Score: 0.2935 | ROUGE-L Score: 0.5196\n",
      "Epoch 30 | Train Loss: 2.3134 | Val Loss: 2.4269\n",
      "BLEU Score: 0.2936 | ROUGE-L Score: 0.5196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1860115296.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss: 2.4103\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR \n",
    "\n",
    "# Learning rate scheduler function\n",
    "def lr_schedule(step):\n",
    "    d_model = config['d_model']\n",
    "    warmup_steps = config['warmup_steps']\n",
    "    step = max(step, 1)  # Prevent step=0 to avoid division by zero\n",
    "    return (d_model ** -0.5) * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
    "\n",
    "# Model Initialization\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size, \n",
    "    tgt_vocab_size,\n",
    "    d_model=config['d_model'],\n",
    "    N=config['n_layers'],\n",
    "    heads=config['heads'],\n",
    "    d_ff=config['d_ff'],\n",
    "    dropout=config['dropout']\n",
    ")\n",
    "model.to(config['device'])\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, pad_idx, smoothing=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.smoothing = smoothing\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        log_pred = F.log_softmax(pred, dim=-1)\n",
    "        non_pad_mask = (target != self.pad_idx)\n",
    "        \n",
    "        # Calculate smoothed target distribution\n",
    "        with torch.no_grad():\n",
    "            smooth_target = torch.full_like(log_pred, self.smoothing / (n_classes - 1))\n",
    "            smooth_target.scatter_(-1, target.unsqueeze(-1), 1 - self.smoothing)\n",
    "            smooth_target.masked_fill_(~non_pad_mask.unsqueeze(-1), 0)  # Zero out padding\n",
    "        \n",
    "        # KL divergence loss\n",
    "        loss = -(smooth_target * log_pred).sum(dim=-1)\n",
    "        loss = loss.masked_fill(~non_pad_mask, 0)  # Ignore padding\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.sum() / non_pad_mask.sum()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], betas=(0.9, 0.98), weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "criterion = LabelSmoothingLoss(tgt_pad_idx, smoothing=config['label_smoothing'])\n",
    "gradient_accumulation_steps = 4\n",
    "# Early stopping settings\n",
    "patience, trials = 5, 0  \n",
    "best_val_loss = float('inf')\n",
    "clip_value = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(config['device']), tgt.to(config['device'])\n",
    "        tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "        output = model(src, tgt_input)\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), tgt_output.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(config['device']), tgt.to(config['device'])\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.view(-1, tgt_vocab_size), tgt_output.contiguous().view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Log losses\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        trials = 0\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    \n",
    "    # Learning rate scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # After validation loss calculation\n",
    "    avg_bleu, avg_rouge_l = evaluate_bleu_rouge_nltk(model, val_loader, english_tokenizer, urdu_tokenizer, config['device'], tgt_pad_idx)\n",
    "    print(f\"BLEU Score: {avg_bleu:.4f} | ROUGE-L Score: {avg_rouge_l:.4f}\")\n",
    "\n",
    "# Test evaluation after training\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "test_loss = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(config['device']), tgt.to(config['device'])\n",
    "        tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "        output = model(src, tgt_input)\n",
    "        loss = criterion(output.view(-1, tgt_vocab_size), tgt_output.contiguous().view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Final Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1526c",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dff6ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf2f7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "# Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attn_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "# Decoder LSTM + Attention\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.lstm = nn.LSTM(hid_dim + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Full Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8416e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = src_vocab_size\n",
    "OUTPUT_DIM = tgt_vocab_size\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(HID_DIM)\n",
    "enc = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model_lstm = Seq2Seq(enc, dec, config['device']).to(config['device'])\n",
    "\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e52dcf69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 31m 13s\n",
      "\tTrain Loss: 6.069\n",
      "\t Val. Loss: 6.123\n",
      "Epoch: 02 | Time: 33m 30s\n",
      "\tTrain Loss: 5.677\n",
      "\t Val. Loss: 6.080\n",
      "Epoch: 03 | Time: 37m 9s\n",
      "\tTrain Loss: 5.456\n",
      "\t Val. Loss: 6.108\n",
      "Epoch: 04 | Time: 37m 28s\n",
      "\tTrain Loss: 5.318\n",
      "\t Val. Loss: 6.102\n",
      "Epoch: 05 | Time: 30m 55s\n",
      "\tTrain Loss: 5.219\n",
      "\t Val. Loss: 6.150\n",
      "Epoch: 06 | Time: 29m 6s\n",
      "\tTrain Loss: 5.110\n",
      "\t Val. Loss: 6.241\n",
      "Epoch: 07 | Time: 31m 23s\n",
      "\tTrain Loss: 5.028\n",
      "\t Val. Loss: 6.418\n",
      "Epoch: 08 | Time: 38m 49s\n",
      "\tTrain Loss: 4.945\n",
      "\t Val. Loss: 6.253\n",
      "Epoch: 09 | Time: 39m 3s\n",
      "\tTrain Loss: 4.865\n",
      "\t Val. Loss: 6.278\n",
      "Epoch: 10 | Time: 33m 13s\n",
      "\tTrain Loss: 4.796\n",
      "\t Val. Loss: 6.271\n",
      "Epoch: 11 | Time: 32m 33s\n",
      "\tTrain Loss: 4.726\n",
      "\t Val. Loss: 6.360\n",
      "Epoch: 12 | Time: 32m 50s\n",
      "\tTrain Loss: 4.638\n",
      "\t Val. Loss: 6.414\n",
      "Epoch: 13 | Time: 38m 50s\n",
      "\tTrain Loss: 4.573\n",
      "\t Val. Loss: 6.480\n",
      "Epoch: 14 | Time: 40m 52s\n",
      "\tTrain Loss: 4.528\n",
      "\t Val. Loss: 6.474\n",
      "Epoch: 15 | Time: 35m 50s\n",
      "\tTrain Loss: 4.483\n",
      "\t Val. Loss: 6.442\n",
      "Epoch: 16 | Time: 29m 8s\n",
      "\tTrain Loss: 4.448\n",
      "\t Val. Loss: 6.464\n",
      "Epoch: 17 | Time: 29m 30s\n",
      "\tTrain Loss: 4.415\n",
      "\t Val. Loss: 6.594\n",
      "Epoch: 18 | Time: 29m 30s\n",
      "\tTrain Loss: 4.423\n",
      "\t Val. Loss: 6.652\n",
      "Epoch: 19 | Time: 29m 24s\n",
      "\tTrain Loss: 4.353\n",
      "\t Val. Loss: 6.855\n",
      "Epoch: 20 | Time: 29m 26s\n",
      "\tTrain Loss: 4.358\n",
      "\t Val. Loss: 6.689\n",
      "Epoch: 21 | Time: 29m 22s\n",
      "\tTrain Loss: 4.313\n",
      "\t Val. Loss: 6.921\n",
      "Epoch: 22 | Time: 29m 14s\n",
      "\tTrain Loss: 4.291\n",
      "\t Val. Loss: 6.843\n",
      "Epoch: 23 | Time: 29m 23s\n",
      "\tTrain Loss: 4.266\n",
      "\t Val. Loss: 6.961\n",
      "Epoch: 24 | Time: 28m 42s\n",
      "\tTrain Loss: 4.199\n",
      "\t Val. Loss: 7.099\n",
      "Epoch: 25 | Time: 34m 5s\n",
      "\tTrain Loss: 4.167\n",
      "\t Val. Loss: 7.052\n",
      "Epoch: 26 | Time: 36m 55s\n",
      "\tTrain Loss: 4.159\n",
      "\t Val. Loss: 7.064\n",
      "Epoch: 27 | Time: 36m 53s\n",
      "\tTrain Loss: 4.116\n",
      "\t Val. Loss: 6.986\n",
      "Epoch: 28 | Time: 37m 33s\n",
      "\tTrain Loss: 4.067\n",
      "\t Val. Loss: 7.171\n",
      "Epoch: 29 | Time: 27m 57s\n",
      "\tTrain Loss: 4.035\n",
      "\t Val. Loss: 7.064\n",
      "Epoch: 30 | Time: 28m 0s\n",
      "\tTrain Loss: 4.034\n",
      "\t Val. Loss: 7.252\n"
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time - (mins * 60))\n",
    "    return mins, secs\n",
    "\n",
    "n_epochs = 30\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model_lstm.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(config['device']), tgt.to(config['device'])\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model_lstm(src, tgt)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_lstm.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model_lstm.eval()\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(config['device']), tgt.to(config['device'])\n",
    "            output = model_lstm(src, tgt, 0)  # no teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(val_loader)\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_lstm.state_dict(), 'best-lstm-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "609a5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\636363076.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_lstm.load_state_dict(torch.load('best-lstm-model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU Score: 0.0137\n",
      "Test ROUGE-L Score: 0.1155\n"
     ]
    }
   ],
   "source": [
    "# After training:\n",
    "model_lstm.load_state_dict(torch.load('best-lstm-model.pt'))\n",
    "bleu_score, rouge_l_score = evaluate_bleu_rouge_nltk(model_lstm, test_loader, english_tokenizer, urdu_tokenizer, config['device'], tgt_pad_idx)\n",
    "print(f\"Test BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"Test ROUGE-L Score: {rouge_l_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecb8fd",
   "metadata": {},
   "source": [
    "#### Visualizing Train+Val Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb7602ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for Transformer\n",
    "transformer_train_losses = [7.56, 6.33, 5.82, 5.40, 5.02, 4.68, 4.38, 4.10, 3.85, 3.63, 3.43, 3.25, 3.09, 2.96, 2.84, 2.74, 2.66, 2.59, 2.53, 2.47, 2.43, 2.40, 2.37, 2.35, 2.34, 2.33, 2.32, 2.31, 2.31, 2.31]\n",
    "transformer_val_losses = [6.70, 6.06, 5.64, 5.22, 4.87, 4.56, 4.29, 4.03, 3.79, 3.58, 3.40, 3.24, 3.10, 2.98, 2.87, 2.79, 2.71, 2.65, 2.60, 2.55, 2.52, 2.49, 2.47, 2.45, 2.44, 2.43, 2.43, 2.42, 2.42, 2.42]\n",
    "\n",
    "# Losses for LSTM\n",
    "lstm_train_losses = [6.06, 6.03, 5.67, 5.45, 5.31, 5.21, 5.11, 5.02, 4.95, 4.86, 4.76, 4.72, 4.63, 4.57, 4.52, 4.48, 4.44, 4.41, 4.42, 4.35, 4.31, 4.29, 4.26, 4.19, 4.16, 4.15, 4.11, 4.06, 4.03, 4.03]\n",
    "lstm_val_losses = [6.12, 6.08, 6.10, 6.10, 6.15, 6.24, 6.41, 6.25, 6.27, 6.27, 6.36, 6.41, 6.48, 6.47, 6.44, 6.46, 6.59, 6.65, 6.85, 6.68, 6.92, 6.84, 6.96, 7.09, 7.05, 6.98, 7.17, 7.06, 7.25, 7.15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0c90e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(transformer_train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Transformer\n",
    "plt.plot(epochs, transformer_train_losses, label='Transformer Train Loss', color='blue')\n",
    "plt.plot(epochs, transformer_val_losses, label='Transformer Val Loss', color='skyblue')\n",
    "\n",
    "# LSTM\n",
    "plt.plot(epochs, lstm_train_losses, label='LSTM Train Loss', color='red')\n",
    "plt.plot(epochs, lstm_val_losses, label='LSTM Val Loss', color='salmon')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves (Transformer vs LSTM)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff819ed4",
   "metadata": {},
   "source": [
    "#### Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9c3a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0, \"d_model must be divisible by heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.attn_weights = None  # <--- ADD THIS\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        if y is None:\n",
    "            y = x\n",
    "\n",
    "        B, L_x, _ = x.size()\n",
    "        L_y = y.size(1)\n",
    "\n",
    "        q = self.q_linear(x).view(B, L_x, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        self.attn_weights = attn.detach().cpu()  # <--- STORE ATTENTION\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_x, self.d_model)\n",
    "        out = self.fc_out(context)\n",
    "        return out\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# Positional Encoding\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "#         self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "#         return x\n",
    "\n",
    "# # Scaled Dot-Product Attention\n",
    "# def scaled_dot_product(q, k, v, mask=None):\n",
    "#     d_k = q.size(-1)\n",
    "#     scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "#     if mask is not None:\n",
    "#         scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "#     attn = torch.softmax(scores, dim=-1)\n",
    "#     output = torch.matmul(attn, v)\n",
    "#     return output, attn\n",
    "\n",
    "# Feed Forward Network\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(d_model, d_ff),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(d_ff, d_model)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# Encoder Layer\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.attn = MultiHeadAttention(heads, d_model)\n",
    "#         self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         x = x + self.dropout(self.attn(self.norm1(x), mask=mask))\n",
    "#         x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "#         return x\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.cross_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), mask=tgt_mask))\n",
    "        cross_output = self.cross_attn(self.norm2(x), y=enc_out, mask=src_mask)\n",
    "        self.attn_weights = self.cross_attn.attn_weights  # <--- ADD THIS\n",
    "        x = x + self.dropout(cross_output)\n",
    "        x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Encoder\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.positional_encoding = PositionalEncoding(d_model)\n",
    "#         self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "#         self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#     def forward(self, src, mask=None):\n",
    "#         x = self.embed(src)\n",
    "#         x = self.positional_encoding(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.attn_maps = None  # ← ADD THIS HERE (ensures it exists even before forward)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = self.embed(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        self.attn_maps = []  # ← Reset this inside forward\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "            self.attn_maps.append(layer.attn_weights)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "\n",
    "# # Full Transformer Model\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Encoder(src_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "#         self.decoder = Decoder(tgt_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "#         self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "#         enc_out = self.encoder(src, src_mask)\n",
    "#         dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "#         out = self.fc_out(dec_out)\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a0af828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\2526784739.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(\"best_model.pt\", map_location=config['device']))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======= IMPORTS =======\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ======= LOAD MODEL MATCHING YOUR CONFIG =======\n",
    "model2 = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=config['d_model'],       # 256\n",
    "    N=config['n_layers'],            # 4\n",
    "    heads=config['heads'],           # 8\n",
    "    d_ff=config['d_ff'],             # 1024\n",
    "    dropout=config['dropout']        # 0.1\n",
    ").to(config['device'])\n",
    "\n",
    "model2.load_state_dict(torch.load(\"best_model.pt\", map_location=config['device']))\n",
    "model2.eval()\n",
    "\n",
    "# ======= TRANSLATE + GET ATTENTION =======\n",
    "def translate_with_attention(model, src_tensor, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.unsqueeze(0).to(config['device'])  # (1, seq_len)\n",
    "    memory = model.encoder(src_tensor)\n",
    "\n",
    "    tgt_input = torch.tensor([[tgt_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "    output_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output = model.decoder(tgt_input, memory)\n",
    "        output = model.fc_out(output)\n",
    "        next_token = output[:, -1, :].argmax(-1)\n",
    "        output_tokens.append(next_token.item())\n",
    "        tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "        if next_token.item() == tgt_tokenizer.token_to_id(\"[SEP]\"):\n",
    "            break\n",
    "\n",
    "    # Get attention weights from last decoder layer\n",
    "    attn_weights = model.decoder.attn_maps[-1]  # (1, heads, tgt_len, src_len)\n",
    "    attn_avg = attn_weights.mean(dim=1).squeeze(0).cpu().detach().numpy()  # (tgt_len, src_len)\n",
    "\n",
    "    return output_tokens, attn_avg\n",
    "\n",
    "# ======= PLOT HEATMAP =======\n",
    "def plot_attention(attention_matrix, source_tokens, target_tokens, title=\"Transformer Attention Heatmap\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(attention_matrix, xticklabels=source_tokens, yticklabels=target_tokens, cmap=\"viridis\")\n",
    "    plt.xlabel(\"English Source Sentence\")\n",
    "    plt.ylabel(\"Urdu Predicted Translation\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# ======= RUN ON ONE TEST EXAMPLE =======\n",
    "sample_src, _ = next(iter(test_loader))  # one batch\n",
    "sample_src = sample_src[0]               # take first sentence\n",
    "\n",
    "output_ids, attn_matrix = translate_with_attention(model2, sample_src, english_tokenizer, urdu_tokenizer)\n",
    "\n",
    "src_tokens = [english_tokenizer.id_to_token(idx) for idx in sample_src.cpu().numpy() if idx != src_pad_idx]\n",
    "pred_tokens = [urdu_tokenizer.id_to_token(idx) for idx in output_ids if idx not in [tgt_pad_idx, urdu_tokenizer.token_to_id(\"[SEP]\")]]\n",
    "\n",
    "plot_attention(attn_matrix, src_tokens, pred_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b762d31",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "07d4cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\423287689.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_model = torch.load('best_model.pt')\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\423287689.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lstm_model = torch.load('best-lstm-model.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the transformer model\n",
    "transformer_model = torch.load('best_model.pt')\n",
    "\n",
    "# Load the LSTM model\n",
    "lstm_model = torch.load('best-lstm-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ec0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\2897147996.py:263: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.transformer_model.load_state_dict(torch.load(\"best_model.pt\", map_location=config['device']))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\2897147996.py:284: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.lstm_model.load_state_dict(torch.load(\"best-lstm-model.pt\", map_location=config['device']))\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, messagebox\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0, \"d_model must be divisible by heads\"\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        if y is None: y = x\n",
    "        B, L_x, _ = x.size()\n",
    "        L_y = y.size(1)\n",
    "        q = self.q_linear(x).view(B, L_x, self.heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_x, self.d_model)\n",
    "        return self.fc_out(context)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask=mask))\n",
    "        x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.cross_attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = x + self.dropout(self.self_attn(self.norm1(x), mask=tgt_mask))\n",
    "        x = x + self.dropout(self.cross_attn(self.norm2(x), y=enc_out, mask=src_mask))\n",
    "        x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        x = self.embed(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "        x = self.embed(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        return self.fc_out(dec_out)\n",
    "\n",
    "# LSTM Model Classes\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attn_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.lstm = nn.LSTM(hid_dim + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'd_model': 256,\n",
    "    'n_layers': 4,\n",
    "    'heads': 8,\n",
    "    'd_ff': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'max_seq_len': 100,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "class TranslationApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"English to Urdu Translator\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        \n",
    "        # Initialize tokenizers and models\n",
    "        self.english_tokenizer = None\n",
    "        self.urdu_tokenizer = None\n",
    "        self.transformer_model = None\n",
    "        self.lstm_model = None\n",
    "        \n",
    "        # Load tokenizers and models\n",
    "        self.load_resources()\n",
    "        \n",
    "        # Create UI elements\n",
    "        self.create_widgets()\n",
    "        \n",
    "        # Store conversation history\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def load_resources(self):\n",
    "        try:\n",
    "            # Load tokenizers (you'll need to replace these paths with your actual tokenizer files)\n",
    "#             self.english_tokenizer = Tokenizer.from_file(\"english_tokenizer.json\")\n",
    "#             self.urdu_tokenizer = Tokenizer.from_file(\"urdu_tokenizer.json\")\n",
    "            \n",
    "#             # Get vocab sizes\n",
    "#             src_vocab_size = self.english_tokenizer.get_vocab_size()\n",
    "#             tgt_vocab_size = self.urdu_tokenizer.get_vocab_size()\n",
    "            \n",
    "            # Initialize and load Transformer model\n",
    "            self.transformer_model = Transformer(\n",
    "                src_vocab_size=src_vocab_size,\n",
    "                tgt_vocab_size=tgt_vocab_size,\n",
    "                d_model=config['d_model'],\n",
    "                N=config['n_layers'],\n",
    "                heads=config['heads'],\n",
    "                d_ff=config['d_ff'],\n",
    "                dropout=config['dropout']\n",
    "            ).to(config['device'])\n",
    "            self.transformer_model.load_state_dict(torch.load(\"best_model.pt\", map_location=config['device']))\n",
    "            self.transformer_model.eval()\n",
    "            \n",
    "            # Initialize and load LSTM model\n",
    "            enc = EncoderLSTM(\n",
    "                input_dim=src_vocab_size,\n",
    "                emb_dim=256,\n",
    "                hid_dim=512,\n",
    "                n_layers=2,\n",
    "                dropout=0.5\n",
    "            )\n",
    "            attn = Attention(512)\n",
    "            dec = DecoderLSTM(\n",
    "                output_dim=tgt_vocab_size,\n",
    "                emb_dim=256,\n",
    "                hid_dim=512,\n",
    "                n_layers=2,\n",
    "                dropout=0.5,\n",
    "                attention=attn\n",
    "            )\n",
    "            self.lstm_model = Seq2Seq(enc, dec, config['device']).to(config['device'])\n",
    "            self.lstm_model.load_state_dict(torch.load(\"best-lstm-model.pt\", map_location=config['device']))\n",
    "            self.lstm_model.eval()\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load resources: {str(e)}\")\n",
    "            self.root.destroy()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        # Main frame\n",
    "        main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Model selection\n",
    "        model_frame = ttk.Frame(main_frame)\n",
    "        model_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "        ttk.Label(model_frame, text=\"Model:\").pack(side=tk.LEFT)\n",
    "        self.model_var = tk.StringVar(value=\"transformer\")\n",
    "        ttk.Radiobutton(model_frame, text=\"Transformer\", variable=self.model_var, value=\"transformer\").pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Radiobutton(model_frame, text=\"LSTM\", variable=self.model_var, value=\"lstm\").pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Conversation display\n",
    "        self.conversation_text = scrolledtext.ScrolledText(\n",
    "            main_frame, \n",
    "            wrap=tk.WORD, \n",
    "            font=('Arial', 12), \n",
    "            state='disabled'\n",
    "        )\n",
    "        self.conversation_text.pack(fill=tk.BOTH, expand=True, pady=10)\n",
    "        \n",
    "        # Configure tags for alignment\n",
    "        self.conversation_text.tag_configure('left', justify='left')\n",
    "        self.conversation_text.tag_configure('right', justify='right')\n",
    "        self.conversation_text.tag_configure('bold', font=('Arial', 12, 'bold'))\n",
    "        \n",
    "        # Input frame\n",
    "        input_frame = ttk.Frame(main_frame)\n",
    "        input_frame.pack(fill=tk.X)\n",
    "        \n",
    "        self.input_entry = ttk.Entry(\n",
    "            input_frame, \n",
    "            font=('Arial', 12)\n",
    "        )\n",
    "        self.input_entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))\n",
    "        self.input_entry.bind(\"<Return>\", self.on_enter_pressed)\n",
    "        \n",
    "        translate_btn = ttk.Button(\n",
    "            input_frame, \n",
    "            text=\"Translate\", \n",
    "            command=self.translate_text\n",
    "        )\n",
    "        translate_btn.pack(side=tk.RIGHT)\n",
    "        \n",
    "        # Clear button\n",
    "        clear_btn = ttk.Button(\n",
    "            main_frame,\n",
    "            text=\"Clear Conversation\",\n",
    "            command=self.clear_conversation\n",
    "        )\n",
    "        clear_btn.pack(pady=5)\n",
    "    \n",
    "    def on_enter_pressed(self, event):\n",
    "        self.translate_text()\n",
    "    \n",
    "    def translate_text(self):\n",
    "        input_text = self.input_entry.get().strip()\n",
    "        if not input_text:\n",
    "            return\n",
    "        \n",
    "        # Add user input to conversation\n",
    "        self.add_to_conversation(\"You\", input_text, align=\"left\")\n",
    "        self.input_entry.delete(0, tk.END)\n",
    "        \n",
    "        try:\n",
    "            # Get translation\n",
    "            translated_text = self.get_translation(input_text)\n",
    "            \n",
    "            # Add translation to conversation\n",
    "            self.add_to_conversation(\"Translator\", translated_text, align=\"right\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Translation Error\", f\"Failed to translate: {str(e)}\")\n",
    "    \n",
    "    def get_translation(self, text):\n",
    "        # Tokenize input\n",
    "        eng_encoded = self.english_tokenizer.encode(text)\n",
    "        src_ids = [self.english_tokenizer.token_to_id(\"[CLS]\")] + \\\n",
    "                 [self.english_tokenizer.token_to_id(t) for t in eng_encoded.tokens \n",
    "                  if self.english_tokenizer.token_to_id(t) is not None] + \\\n",
    "                 [self.english_tokenizer.token_to_id(\"[SEP]\")]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(config['device'])\n",
    "        \n",
    "        # Get model\n",
    "        model = self.transformer_model if self.model_var.get() == \"transformer\" else self.lstm_model\n",
    "        \n",
    "        # Generate translation\n",
    "        if self.model_var.get() == \"transformer\":\n",
    "            # Transformer translation\n",
    "            memory = model.encoder(src_tensor)\n",
    "            tgt_input = torch.tensor([[self.urdu_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "            \n",
    "            output_tokens = []\n",
    "            for _ in range(config['max_seq_len']):\n",
    "                output = model.decoder(tgt_input, memory)\n",
    "                output = model.fc_out(output)\n",
    "                next_token = output[:, -1, :].argmax(-1)\n",
    "                output_tokens.append(next_token.item())\n",
    "                tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "                \n",
    "                if next_token.item() == self.urdu_tokenizer.token_to_id(\"[SEP]\"):\n",
    "                    break\n",
    "        else:\n",
    "            # LSTM translation\n",
    "            encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "            tgt_input = torch.tensor([[self.urdu_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "            \n",
    "            output_tokens = []\n",
    "            for _ in range(config['max_seq_len']):\n",
    "                output, hidden, cell = model.decoder(\n",
    "                    tgt_input[:, -1], hidden, cell, encoder_outputs\n",
    "                )\n",
    "                next_token = output.argmax(-1)\n",
    "                output_tokens.append(next_token.item())\n",
    "                tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "                \n",
    "                if next_token.item() == self.urdu_tokenizer.token_to_id(\"[SEP]\"):\n",
    "                    break\n",
    "        \n",
    "        # Convert output tokens to text\n",
    "        translated_text = \" \".join(\n",
    "            [self.urdu_tokenizer.id_to_token(idx) for idx in output_tokens \n",
    "             if idx not in [self.urdu_tokenizer.token_to_id(\"[PAD]\"), \n",
    "                          self.urdu_tokenizer.token_to_id(\"[SEP]\")]]\n",
    "        )\n",
    "        \n",
    "        return translated_text\n",
    "    \n",
    "    def add_to_conversation(self, sender, text, align=\"left\"):\n",
    "        self.conversation_text.configure(state='normal')\n",
    "        \n",
    "        # Insert sender name\n",
    "        self.conversation_text.insert(tk.END, f\"{sender}:\\n\", ('bold', align))\n",
    "        \n",
    "        # Insert message text\n",
    "        self.conversation_text.insert(tk.END, f\"{text}\\n\\n\", (align,))\n",
    "        \n",
    "        self.conversation_text.configure(state='disabled')\n",
    "        self.conversation_text.see(tk.END)\n",
    "        \n",
    "        # Store in history\n",
    "        self.conversation_history.append((sender, text, align))\n",
    "    \n",
    "    def clear_conversation(self):\n",
    "        self.conversation_text.configure(state='normal')\n",
    "        self.conversation_text.delete(1.0, tk.END)\n",
    "        self.conversation_text.configure(state='disabled')\n",
    "        self.conversation_history = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = TranslationApp(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16a5b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_model = torch.load('best_model.pt')\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lstm_model = torch.load('best-lstm-model.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models would be loaded here in a real implementation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 86, in on_enter_pressed\n",
      "    self.translate_text()\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 98, in translate_text\n",
      "    translated_text = self.get_translation(input_text)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 134, in get_translation\n",
      "    memory = model.encoder(src_tensor)\n",
      "AttributeError: 'NoneType' object has no attribute 'encoder'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 86, in on_enter_pressed\n",
      "    self.translate_text()\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 98, in translate_text\n",
      "    translated_text = self.get_translation(input_text)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 134, in get_translation\n",
      "    memory = model.encoder(src_tensor)\n",
      "AttributeError: 'NoneType' object has no attribute 'encoder'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 86, in on_enter_pressed\n",
      "    self.translate_text()\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 98, in translate_text\n",
      "    translated_text = self.get_translation(input_text)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\1031892284.py\", line 134, in get_translation\n",
      "    memory = model.encoder(src_tensor)\n",
      "AttributeError: 'NoneType' object has no attribute 'encoder'\n"
     ]
    }
   ],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import ttk, scrolledtext, messagebox\n",
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# class TranslationApp:\n",
    "#     def __init__(self, root):\n",
    "#         self.root = root\n",
    "#         self.root.title(\"English to Urdu Translator\")\n",
    "#         self.root.geometry(\"800x600\")\n",
    "        \n",
    "#         # Load models (replace with your actual model loading code)\n",
    "#         self.transformer_model = None\n",
    "#         self.lstm_model = None\n",
    "#         self.load_models()\n",
    "        \n",
    "#         # Create UI elements\n",
    "#         self.create_widgets()\n",
    "        \n",
    "#         # Store conversation history\n",
    "#         self.conversation_history = []\n",
    "    \n",
    "#     def load_models(self):\n",
    "#         try:\n",
    "#             # Load the transformer model\n",
    "#             transformer_model = torch.load('best_model.pt')\n",
    "\n",
    "#             # Load the LSTM model\n",
    "#             lstm_model = torch.load('best-lstm-model.pt')\n",
    "            \n",
    "#             # For now, we'll just print a message\n",
    "#             print(\"Models would be loaded here in a real implementation\")\n",
    "#         except Exception as e:\n",
    "#             messagebox.showerror(\"Error\", f\"Failed to load models: {str(e)}\")\n",
    "    \n",
    "#     def create_widgets(self):\n",
    "#         # Main frame\n",
    "#         main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "#         main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "#         # Model selection\n",
    "#         model_frame = ttk.Frame(main_frame)\n",
    "#         model_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "#         ttk.Label(model_frame, text=\"Model:\").pack(side=tk.LEFT)\n",
    "#         self.model_var = tk.StringVar(value=\"transformer\")\n",
    "#         ttk.Radiobutton(model_frame, text=\"Transformer\", variable=self.model_var, value=\"transformer\").pack(side=tk.LEFT, padx=5)\n",
    "#         ttk.Radiobutton(model_frame, text=\"LSTM\", variable=self.model_var, value=\"lstm\").pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "#         # Conversation display\n",
    "#         self.conversation_text = scrolledtext.ScrolledText(\n",
    "#             main_frame, \n",
    "#             wrap=tk.WORD, \n",
    "#             font=('Arial', 12), \n",
    "#             state='disabled'\n",
    "#         )\n",
    "#         self.conversation_text.pack(fill=tk.BOTH, expand=True, pady=10)\n",
    "        \n",
    "#         # Input frame\n",
    "#         input_frame = ttk.Frame(main_frame)\n",
    "#         input_frame.pack(fill=tk.X)\n",
    "        \n",
    "#         self.input_entry = ttk.Entry(\n",
    "#             input_frame, \n",
    "#             font=('Arial', 12)\n",
    "#         )\n",
    "#         self.input_entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))\n",
    "#         self.input_entry.bind(\"<Return>\", self.on_enter_pressed)\n",
    "        \n",
    "#         translate_btn = ttk.Button(\n",
    "#             input_frame, \n",
    "#             text=\"Translate\", \n",
    "#             command=self.translate_text\n",
    "#         )\n",
    "#         translate_btn.pack(side=tk.RIGHT)\n",
    "        \n",
    "#         # Clear button\n",
    "#         clear_btn = ttk.Button(\n",
    "#             main_frame,\n",
    "#             text=\"Clear Conversation\",\n",
    "#             command=self.clear_conversation\n",
    "#         )\n",
    "#         clear_btn.pack(pady=5)\n",
    "    \n",
    "#     def on_enter_pressed(self, event):\n",
    "#         self.translate_text()\n",
    "    \n",
    "#     def translate_text(self):\n",
    "#         input_text = self.input_entry.get().strip()\n",
    "#         if not input_text:\n",
    "#             return\n",
    "        \n",
    "#         # Add user input to conversation\n",
    "#         self.add_to_conversation(\"You\", input_text, align=\"left\")\n",
    "#         self.input_entry.delete(0, tk.END)\n",
    "        \n",
    "#         # Get translation (in a real app, this would use your actual model)\n",
    "#         translated_text = self.get_translation(input_text)\n",
    "        \n",
    "#         # Add translation to conversation\n",
    "#         self.add_to_conversation(\"Translator\", translated_text, align=\"right\")\n",
    "    \n",
    "# #     def get_translation(self, text):\n",
    "# #         # In a real implementation, this would:\n",
    "# #         # 1. Tokenize the input text using english_tokenizer\n",
    "# #         # 2. Convert to tensor and move to device\n",
    "# #         # 3. Run through the selected model (transformer or LSTM)\n",
    "# #         # 4. Decode the output using urdu_tokenizer\n",
    "# #         # 5. Return the translated text\n",
    "        \n",
    "# #         # For demonstration, we'll just return a placeholder\n",
    "# #         model_type = self.model_var.get()\n",
    "# #         if model_type == \"transformer\":\n",
    "# #             return \"ٹرانسفارمر ماڈل کا ترجمہ: \" + text[::-1]  # Reversed as placeholder\n",
    "# #         else:\n",
    "# #             return \"LSTM ماڈل کا ترجمہ: \" + text[::-1]  # Reversed as placeholder\n",
    "\n",
    "#     def get_translation(self, text):\n",
    "#         # Tokenize input\n",
    "#         eng_encoded = english_tokenizer.encode(text)\n",
    "#         src_ids = [english_tokenizer.token_to_id(\"[CLS]\")] + \\\n",
    "#                   [english_tokenizer.token_to_id(t) for t in eng_encoded.tokens if english_tokenizer.token_to_id(t) is not None] + \\\n",
    "#                   [english_tokenizer.token_to_id(\"[SEP]\")]\n",
    "\n",
    "#         # Convert to tensor\n",
    "#         src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(config['device'])\n",
    "\n",
    "#         # Get model\n",
    "#         model = self.transformer_model if self.model_var.get() == \"transformer\" else self.lstm_model\n",
    "\n",
    "#         # Generate translation\n",
    "#         if self.model_var.get() == \"transformer\":\n",
    "#             # Transformer translation logic\n",
    "#             memory = model.encoder(src_tensor)\n",
    "#             tgt_input = torch.tensor([[urdu_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "\n",
    "#             output_tokens = []\n",
    "#             for _ in range(config['max_seq_len']):\n",
    "#                 output = model.decoder(tgt_input, memory)\n",
    "#                 output = model.fc_out(output)\n",
    "#                 next_token = output[:, -1, :].argmax(-1)\n",
    "#                 output_tokens.append(next_token.item())\n",
    "#                 tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "\n",
    "#                 if next_token.item() == urdu_tokenizer.token_to_id(\"[SEP]\"):\n",
    "#                     break\n",
    "#         else:\n",
    "#             # LSTM translation logic\n",
    "#             output_tokens = []\n",
    "#             # ... (LSTM-specific generation code)\n",
    "\n",
    "#         # Convert output tokens to text\n",
    "#         translated_text = \" \".join([urdu_tokenizer.id_to_token(idx) for idx in output_tokens \n",
    "#                                    if idx not in [tgt_pad_idx, urdu_tokenizer.token_to_id(\"[SEP]\")]])\n",
    "\n",
    "#         return translated_text\n",
    "    \n",
    "#     def add_to_conversation(self, sender, text, align=\"left\"):\n",
    "#         self.conversation_text.configure(state='normal')\n",
    "        \n",
    "#         # Configure tag for alignment\n",
    "#         self.conversation_text.tag_configure(align, justify=align)\n",
    "        \n",
    "#         # Insert sender name\n",
    "#         self.conversation_text.insert(tk.END, f\"{sender}:\\n\", ('bold', align))\n",
    "        \n",
    "#         # Insert message text\n",
    "#         self.conversation_text.insert(tk.END, f\"{text}\\n\\n\", (align,))\n",
    "        \n",
    "#         self.conversation_text.configure(state='disabled')\n",
    "#         self.conversation_text.see(tk.END)\n",
    "        \n",
    "#         # Store in history\n",
    "#         self.conversation_history.append((sender, text, align))\n",
    "    \n",
    "#     def clear_conversation(self):\n",
    "#         self.conversation_text.configure(state='normal')\n",
    "#         self.conversation_text.delete(1.0, tk.END)\n",
    "#         self.conversation_text.configure(state='disabled')\n",
    "#         self.conversation_history = []\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = tk.Tk()\n",
    "#     app = TranslationApp(root)\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6d1f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\432506357.py:331: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\"best_model.pt\", map_location=config['device'])\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_21952\\432506357.py:367: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.lstm_model.load_state_dict(torch.load(\"best-lstm-model.pt\", map_location=config['device']))\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "NULL main window",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 528>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     root \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mTk()\n\u001b[1;32m--> 530\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43mTranslationApp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m     root\u001b[38;5;241m.\u001b[39mmainloop()\n",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36mTranslationApp.__init__\u001b[1;34m(self, root)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_resources()\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Create UI elements\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_widgets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Store conversation history\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36mTranslationApp.create_widgets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_widgets\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# Main frame\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     main_frame \u001b[38;5;241m=\u001b[39m \u001b[43mttk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m     main_frame\u001b[38;5;241m.\u001b[39mpack(fill\u001b[38;5;241m=\u001b[39mtk\u001b[38;5;241m.\u001b[39mBOTH, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# Model selection\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\ttk.py:735\u001b[0m, in \u001b[0;36mFrame.__init__\u001b[1;34m(self, master, **kw)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, master\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct a Ttk Frame with parent master.\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    STANDARD OPTIONS\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m        borderwidth, relief, padding, width, height\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m     \u001b[43mWidget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mttk::frame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\ttk.py:552\u001b[0m, in \u001b[0;36mWidget.__init__\u001b[1;34m(self, master, widgetname, kw)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(master, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tile_loaded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;66;03m# Load tile now, if needed\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     _load_tile(master)\n\u001b[1;32m--> 552\u001b[0m \u001b[43mtkinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWidget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidgetname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tkinter\\__init__.py:2572\u001b[0m, in \u001b[0;36mBaseWidget.__init__\u001b[1;34m(self, master, widgetName, cnf, kw, extra)\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cnf[k]\n\u001b[1;32m-> 2572\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidgetName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[0;32m   2575\u001b[0m     k\u001b[38;5;241m.\u001b[39mconfigure(\u001b[38;5;28mself\u001b[39m, v)\n",
      "\u001b[1;31mTclError\u001b[0m: NULL main window"
     ]
    }
   ],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import ttk, scrolledtext, messagebox\n",
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# import math\n",
    "# import os\n",
    "# from tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers, decoders, processors\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe.unsqueeze(0))  # Shape: (1, max_len, d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x shape: (batch_size, seq_len, d_model)\n",
    "#         return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "# # [Keep all your model classes the same as before: MultiHeadAttention, FeedForward, \n",
    "# # EncoderLayer, DecoderLayer, Encoder, Decoder, Transformer, EncoderLSTM, \n",
    "# # Attention, DecoderLSTM, Seq2Seq]\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, heads, d_model):\n",
    "#         super().__init__()\n",
    "#         assert d_model % heads == 0, \"d_model must be divisible by heads\"\n",
    "#         self.d_model = d_model\n",
    "#         self.heads = heads\n",
    "#         self.d_k = d_model // heads\n",
    "#         self.q_linear = nn.Linear(d_model, d_model)\n",
    "#         self.k_linear = nn.Linear(d_model, d_model)\n",
    "#         self.v_linear = nn.Linear(d_model, d_model)\n",
    "#         self.fc_out = nn.Linear(d_model, d_model)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x, y=None, mask=None):\n",
    "#         if y is None: y = x\n",
    "#         B, L_x, _ = x.size()\n",
    "#         L_y = y.size(1)\n",
    "#         q = self.q_linear(x).view(B, L_x, self.heads, self.d_k).transpose(1, 2)\n",
    "#         k = self.k_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "#         v = self.v_linear(y).view(B, L_y, self.heads, self.d_k).transpose(1, 2)\n",
    "#         scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "#             scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "#         attn = torch.softmax(scores, dim=-1)\n",
    "#         attn = self.dropout(attn)\n",
    "#         context = torch.matmul(attn, v)\n",
    "#         context = context.transpose(1, 2).contiguous().view(B, L_x, self.d_model)\n",
    "#         return self.fc_out(context)\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(d_model, d_ff),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(d_ff, d_model)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.attn = MultiHeadAttention(heads, d_model)\n",
    "#         self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         x = x + self.dropout(self.attn(self.norm1(x), mask=mask))\n",
    "#         x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "#         return x\n",
    "\n",
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = MultiHeadAttention(heads, d_model)\n",
    "#         self.cross_attn = MultiHeadAttention(heads, d_model)\n",
    "#         self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.norm3 = nn.LayerNorm(d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "#         x = x + self.dropout(self.self_attn(self.norm1(x), mask=tgt_mask))\n",
    "#         x = x + self.dropout(self.cross_attn(self.norm2(x), y=enc_out, mask=src_mask))\n",
    "#         x = x + self.dropout(self.ff(self.norm3(x)))\n",
    "#         return x\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.positional_encoding = PositionalEncoding(d_model)\n",
    "#         self.layers = nn.ModuleList([EncoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "#         self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#     def forward(self, src, mask=None):\n",
    "#         x = self.embed(src)\n",
    "#         x = self.positional_encoding(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.embed = nn.Embedding(vocab_size, d_model)\n",
    "#         self.positional_encoding = PositionalEncoding(d_model)\n",
    "#         self.layers = nn.ModuleList([DecoderLayer(d_model, heads, d_ff, dropout) for _ in range(N)])\n",
    "#         self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "#     def forward(self, tgt, enc_out, src_mask=None, tgt_mask=None):\n",
    "#         x = self.embed(tgt)\n",
    "#         x = self.positional_encoding(x)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "#         return self.norm(x)\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.encoder = Encoder(src_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "#         self.decoder = Decoder(tgt_vocab_size, d_model, N, heads, d_ff, dropout)\n",
    "#         self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "#     def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "#         enc_out = self.encoder(src, src_mask)\n",
    "#         dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "#         return self.fc_out(dec_out)\n",
    "\n",
    "# # LSTM Model Classes\n",
    "# class EncoderLSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "#         self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         embedded = self.embedding(src)\n",
    "#         outputs, (hidden, cell) = self.lstm(embedded)\n",
    "#         return outputs, hidden, cell\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hid_dim):\n",
    "#         super().__init__()\n",
    "#         self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "#         self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "\n",
    "#     def forward(self, hidden, encoder_outputs):\n",
    "#         src_len = encoder_outputs.shape[1]\n",
    "#         hidden = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)\n",
    "#         energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "#         energy = energy.permute(0, 2, 1)\n",
    "#         v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "#         attn_weights = torch.bmm(v, energy).squeeze(1)\n",
    "#         return torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "# class DecoderLSTM(nn.Module):\n",
    "#     def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "#         super().__init__()\n",
    "#         self.output_dim = output_dim\n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "#         self.attention = attention\n",
    "#         self.lstm = nn.LSTM(hid_dim + emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "#         self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
    "\n",
    "#     def forward(self, input, hidden, cell, encoder_outputs):\n",
    "#         input = input.unsqueeze(1)\n",
    "#         embedded = self.embedding(input)\n",
    "#         attn_weights = self.attention(hidden, encoder_outputs)\n",
    "#         attn_weights = attn_weights.unsqueeze(1)\n",
    "#         context = torch.bmm(attn_weights, encoder_outputs)\n",
    "#         rnn_input = torch.cat((embedded, context), dim=2)\n",
    "#         output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "#         prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "#         return prediction, hidden, cell\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, device):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "#         batch_size = src.size(0)\n",
    "#         trg_len = trg.size(1)\n",
    "#         trg_vocab_size = self.decoder.output_dim\n",
    "#         outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "#         encoder_outputs, hidden, cell = self.encoder(src)\n",
    "#         input = trg[:, 0]\n",
    "\n",
    "#         for t in range(1, trg_len):\n",
    "#             output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "#             outputs[:, t] = output\n",
    "#             top1 = output.argmax(1)\n",
    "#             input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        \n",
    "#         return outputs\n",
    "\n",
    "# # Configuration\n",
    "# config = {\n",
    "#     'd_model': 256,\n",
    "#     'n_layers': 4,\n",
    "#     'heads': 8,\n",
    "#     'd_ff': 1024,\n",
    "#     'dropout': 0.1,\n",
    "#     'batch_size': 32,\n",
    "#     'max_seq_len': 100,\n",
    "#     'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# }\n",
    "\n",
    "# class TranslationApp:\n",
    "#     def __init__(self, root):\n",
    "#         self.root = root\n",
    "#         self.root.title(\"English to Urdu Translator\")\n",
    "#         self.root.geometry(\"800x600\")\n",
    "        \n",
    "#         # Initialize tokenizers and models\n",
    "#         self.english_tokenizer = None\n",
    "#         self.urdu_tokenizer = None\n",
    "#         self.transformer_model = None\n",
    "#         self.lstm_model = None\n",
    "        \n",
    "#         # Load tokenizers and models\n",
    "#         self.load_resources()\n",
    "        \n",
    "#         # Create UI elements\n",
    "#         self.create_widgets()\n",
    "        \n",
    "#         # Store conversation history\n",
    "#         self.conversation_history = []\n",
    "    \n",
    "#     def train_tokenizer(self, language):\n",
    "#         \"\"\"Train a tokenizer for the given language using the corpus file\"\"\"\n",
    "#         tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        \n",
    "#         # Customize tokenizer\n",
    "#         tokenizer.normalizer = normalizers.Sequence([\n",
    "#             normalizers.NFD(),\n",
    "#             normalizers.Lowercase(),\n",
    "#             normalizers.StripAccents()\n",
    "#         ])\n",
    "        \n",
    "#         tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "#             pre_tokenizers.Whitespace(),\n",
    "#             pre_tokenizers.Punctuation()\n",
    "#         ])\n",
    "        \n",
    "#         tokenizer.decoder = decoders.BPEDecoder()\n",
    "        \n",
    "#         # Initialize trainer\n",
    "#         trainer = BpeTrainer(\n",
    "#             special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "#             min_frequency=2,\n",
    "#             show_progress=True,\n",
    "#             vocab_size=8000,\n",
    "#         )\n",
    "        \n",
    "#         # Train tokenizer\n",
    "#         corpus_file = f\"tokenizer_corpus/{language}.txt\"\n",
    "#         tokenizer.train(files=[corpus_file], trainer=trainer)\n",
    "        \n",
    "#         # Add post-processor for [CLS] and [SEP]\n",
    "#         tokenizer.post_processor = processors.TemplateProcessing(\n",
    "#             single=\"[CLS] $A [SEP]\",\n",
    "#             pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "#             special_tokens=[\n",
    "#                 (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "#                 (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "#             ],\n",
    "#         )\n",
    "        \n",
    "#         return tokenizer\n",
    "    \n",
    "#     def load_resources(self):\n",
    "#         try:\n",
    "#             # Create tokenizer_corpus directory if it doesn't exist\n",
    "#             os.makedirs(\"tokenizer_corpus\", exist_ok=True)\n",
    "            \n",
    "#             # Check if tokenizer files exist\n",
    "#             if not os.path.exists(\"tokenizer_corpus/english.txt\"):\n",
    "#                 raise FileNotFoundError(\"English corpus file not found in tokenizer_corpus folder\")\n",
    "#             if not os.path.exists(\"tokenizer_corpus/urdu.txt\"):\n",
    "#                 raise FileNotFoundError(\"Urdu corpus file not found in tokenizer_corpus folder\")\n",
    "            \n",
    "#             # Train or load tokenizers\n",
    "#             if not os.path.exists(\"english_tokenizer.json\"):\n",
    "#                 self.english_tokenizer = self.train_tokenizer(\"english\")\n",
    "#                 self.english_tokenizer.save(\"english_tokenizer.json\")\n",
    "#             else:\n",
    "#                 self.english_tokenizer = Tokenizer.from_file(\"english_tokenizer.json\")\n",
    "                \n",
    "#             if not os.path.exists(\"urdu_tokenizer.json\"):\n",
    "#                 self.urdu_tokenizer = self.train_tokenizer(\"urdu\")\n",
    "#                 self.urdu_tokenizer.save(\"urdu_tokenizer.json\")\n",
    "#             else:\n",
    "#                 self.urdu_tokenizer = Tokenizer.from_file(\"urdu_tokenizer.json\")\n",
    "            \n",
    "#             src_vocab_size = self.english_tokenizer.get_vocab_size()\n",
    "#             tgt_vocab_size = self.urdu_tokenizer.get_vocab_size()\n",
    "\n",
    "#             # Initialize model with current vocab sizes\n",
    "#             self.transformer_model = Transformer(\n",
    "#                 src_vocab_size=src_vocab_size,\n",
    "#                 tgt_vocab_size=tgt_vocab_size,\n",
    "#                 d_model=config['d_model'],\n",
    "#                 N=config['n_layers'],\n",
    "#                 heads=config['heads'],\n",
    "#                 d_ff=config['d_ff'],\n",
    "#                 dropout=config['dropout']\n",
    "#             ).to(config['device'])\n",
    "\n",
    "#             # Load pretrained weights with strict=False to handle size mismatches\n",
    "#             if os.path.exists(\"best_model.pt\"):\n",
    "#                 pretrained_dict = torch.load(\"best_model.pt\", map_location=config['device'])\n",
    "#                 model_dict = self.transformer_model.state_dict()\n",
    "\n",
    "#                 # 1. Filter out unnecessary keys\n",
    "#                 pretrained_dict = {k: v for k, v in pretrained_dict.items() \n",
    "#                                   if k in model_dict and v.size() == model_dict[k].size()}\n",
    "\n",
    "#                 # 2. Overwrite entries in the existing state dict\n",
    "#                 model_dict.update(pretrained_dict)\n",
    "\n",
    "#                 # 3. Load the modified state dict\n",
    "#                 self.transformer_model.load_state_dict(model_dict, strict=False)\n",
    "\n",
    "#             self.transformer_model.eval()\n",
    "            \n",
    "#             # Initialize and load LSTM model\n",
    "#             enc = EncoderLSTM(\n",
    "#                 input_dim=src_vocab_size,\n",
    "#                 emb_dim=256,\n",
    "#                 hid_dim=512,\n",
    "#                 n_layers=2,\n",
    "#                 dropout=0.5\n",
    "#             )\n",
    "#             attn = Attention(512)\n",
    "#             dec = DecoderLSTM(\n",
    "#                 output_dim=tgt_vocab_size,\n",
    "#                 emb_dim=256,\n",
    "#                 hid_dim=512,\n",
    "#                 n_layers=2,\n",
    "#                 dropout=0.5,\n",
    "#                 attention=attn\n",
    "#             )\n",
    "#             self.lstm_model = Seq2Seq(enc, dec, config['device']).to(config['device'])\n",
    "            \n",
    "#             # Load model weights if they exist\n",
    "#             if os.path.exists(\"best-lstm-model.pt\"):\n",
    "#                 self.lstm_model.load_state_dict(torch.load(\"best-lstm-model.pt\", map_location=config['device']))\n",
    "#             self.lstm_model.eval()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             messagebox.showerror(\"Error\", f\"Failed to load resources: {str(e)}\")\n",
    "#             self.root.destroy()\n",
    "    \n",
    "#     # [Keep all the UI-related methods the same as before: create_widgets, on_enter_pressed, \n",
    "#     # translate_text, get_translation, add_to_conversation, clear_conversation]\n",
    "#     def create_widgets(self):\n",
    "#         # Main frame\n",
    "#         main_frame = ttk.Frame(self.root, padding=\"10\")\n",
    "#         main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "#         # Model selection\n",
    "#         model_frame = ttk.Frame(main_frame)\n",
    "#         model_frame.pack(fill=tk.X, pady=5)\n",
    "        \n",
    "#         ttk.Label(model_frame, text=\"Model:\").pack(side=tk.LEFT)\n",
    "#         self.model_var = tk.StringVar(value=\"transformer\")\n",
    "#         ttk.Radiobutton(model_frame, text=\"Transformer\", variable=self.model_var, value=\"transformer\").pack(side=tk.LEFT, padx=5)\n",
    "#         ttk.Radiobutton(model_frame, text=\"LSTM\", variable=self.model_var, value=\"lstm\").pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "#         # Conversation display\n",
    "#         self.conversation_text = scrolledtext.ScrolledText(\n",
    "#             main_frame, \n",
    "#             wrap=tk.WORD, \n",
    "#             font=('Arial', 12), \n",
    "#             state='disabled'\n",
    "#         )\n",
    "#         self.conversation_text.pack(fill=tk.BOTH, expand=True, pady=10)\n",
    "        \n",
    "#         # Configure tags for alignment\n",
    "#         self.conversation_text.tag_configure('left', justify='left')\n",
    "#         self.conversation_text.tag_configure('right', justify='right')\n",
    "#         self.conversation_text.tag_configure('bold', font=('Arial', 12, 'bold'))\n",
    "        \n",
    "#         # Input frame\n",
    "#         input_frame = ttk.Frame(main_frame)\n",
    "#         input_frame.pack(fill=tk.X)\n",
    "        \n",
    "#         self.input_entry = ttk.Entry(\n",
    "#             input_frame, \n",
    "#             font=('Arial', 12)\n",
    "#         )\n",
    "#         self.input_entry.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))\n",
    "#         self.input_entry.bind(\"<Return>\", self.on_enter_pressed)\n",
    "        \n",
    "#         translate_btn = ttk.Button(\n",
    "#             input_frame, \n",
    "#             text=\"Translate\", \n",
    "#             command=self.translate_text\n",
    "#         )\n",
    "#         translate_btn.pack(side=tk.RIGHT)\n",
    "        \n",
    "#         # Clear button\n",
    "#         clear_btn = ttk.Button(\n",
    "#             main_frame,\n",
    "#             text=\"Clear Conversation\",\n",
    "#             command=self.clear_conversation\n",
    "#         )\n",
    "#         clear_btn.pack(pady=5)\n",
    "    \n",
    "#     def on_enter_pressed(self, event):\n",
    "#         self.translate_text()\n",
    "    \n",
    "#     def translate_text(self):\n",
    "#         input_text = self.input_entry.get().strip()\n",
    "#         if not input_text:\n",
    "#             return\n",
    "        \n",
    "#         # Add user input to conversation\n",
    "#         self.add_to_conversation(\"You\", input_text, align=\"left\")\n",
    "#         self.input_entry.delete(0, tk.END)\n",
    "        \n",
    "#         try:\n",
    "#             # Get translation\n",
    "#             translated_text = self.get_translation(input_text)\n",
    "            \n",
    "#             # Add translation to conversation\n",
    "#             self.add_to_conversation(\"Translator\", translated_text, align=\"right\")\n",
    "#         except Exception as e:\n",
    "#             messagebox.showerror(\"Translation Error\", f\"Failed to translate: {str(e)}\")\n",
    "    \n",
    "#     def get_translation(self, text):\n",
    "#         # Tokenize input\n",
    "#         eng_encoded = self.english_tokenizer.encode(text)\n",
    "#         src_ids = [self.english_tokenizer.token_to_id(\"[CLS]\")] + \\\n",
    "#                  [self.english_tokenizer.token_to_id(t) for t in eng_encoded.tokens \n",
    "#                   if self.english_tokenizer.token_to_id(t) is not None] + \\\n",
    "#                  [self.english_tokenizer.token_to_id(\"[SEP]\")]\n",
    "        \n",
    "#         # Convert to tensor\n",
    "#         src_tensor = torch.LongTensor(src_ids).unsqueeze(0).to(config['device'])\n",
    "        \n",
    "#         # Get model\n",
    "#         model = self.transformer_model if self.model_var.get() == \"transformer\" else self.lstm_model\n",
    "        \n",
    "#         # Generate translation\n",
    "#         if self.model_var.get() == \"transformer\":\n",
    "#             # Transformer translation\n",
    "#             memory = model.encoder(src_tensor)\n",
    "#             tgt_input = torch.tensor([[self.urdu_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "            \n",
    "#             output_tokens = []\n",
    "#             for _ in range(config['max_seq_len']):\n",
    "#                 output = model.decoder(tgt_input, memory)\n",
    "#                 output = model.fc_out(output)\n",
    "#                 next_token = output[:, -1, :].argmax(-1)\n",
    "#                 output_tokens.append(next_token.item())\n",
    "#                 tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "                \n",
    "#                 if next_token.item() == self.urdu_tokenizer.token_to_id(\"[SEP]\"):\n",
    "#                     break\n",
    "#         else:\n",
    "#             # LSTM translation\n",
    "#             encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "#             tgt_input = torch.tensor([[self.urdu_tokenizer.token_to_id(\"[CLS]\")]]).to(config['device'])\n",
    "            \n",
    "#             output_tokens = []\n",
    "#             for _ in range(config['max_seq_len']):\n",
    "#                 output, hidden, cell = model.decoder(\n",
    "#                     tgt_input[:, -1], hidden, cell, encoder_outputs\n",
    "#                 )\n",
    "#                 next_token = output.argmax(-1)\n",
    "#                 output_tokens.append(next_token.item())\n",
    "#                 tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)\n",
    "                \n",
    "#                 if next_token.item() == self.urdu_tokenizer.token_to_id(\"[SEP]\"):\n",
    "#                     break\n",
    "        \n",
    "#         # Convert output tokens to text\n",
    "#         translated_text = \" \".join(\n",
    "#             [self.urdu_tokenizer.id_to_token(idx) for idx in output_tokens \n",
    "#              if idx not in [self.urdu_tokenizer.token_to_id(\"[PAD]\"), \n",
    "#                           self.urdu_tokenizer.token_to_id(\"[SEP]\")]]\n",
    "#         )\n",
    "        \n",
    "#         return translated_text\n",
    "    \n",
    "#     def add_to_conversation(self, sender, text, align=\"left\"):\n",
    "#         self.conversation_text.configure(state='normal')\n",
    "        \n",
    "#         # Insert sender name\n",
    "#         self.conversation_text.insert(tk.END, f\"{sender}:\\n\", ('bold', align))\n",
    "        \n",
    "#         # Insert message text\n",
    "#         self.conversation_text.insert(tk.END, f\"{text}\\n\\n\", (align,))\n",
    "        \n",
    "#         self.conversation_text.configure(state='disabled')\n",
    "#         self.conversation_text.see(tk.END)\n",
    "        \n",
    "#         # Store in history\n",
    "#         self.conversation_history.append((sender, text, align))\n",
    "    \n",
    "#     def clear_conversation(self):\n",
    "#         self.conversation_text.configure(state='normal')\n",
    "#         self.conversation_text.delete(1.0, tk.END)\n",
    "#         self.conversation_text.configure(state='disabled')\n",
    "#         self.conversation_history = []\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     root = tk.Tk()\n",
    "#     app = TranslationApp(root)\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab05af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
